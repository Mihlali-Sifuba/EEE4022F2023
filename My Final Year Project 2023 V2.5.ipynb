{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMpGQgClB5T2U1dif2+vmlx"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"JNeFH7Sfj-VN"},"outputs":[],"source":["!pip install tensorflow-addons"]},{"cell_type":"code","source":["import cv2\n","import os\n","import tensorflow as tf\n","import tensorflow_addons as tfa\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras.layers import Activation, MaxPool2D, Concatenate\n","from tensorflow.keras.losses import MeanSquaredError\n","import imageio.v3 as imageio\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from google.colab import drive"],"metadata":{"id":"mdi9pAdekWS0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Accessing My Google Drive\n","drive.mount('/content/drive')"],"metadata":{"id":"iKJwqUgykZWx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Get $I_1$, $I_{1}^w$ and $I_2$ images"],"metadata":{"id":"aKp9hVPqk4o_"}},{"cell_type":"code","source":["path = os.getcwd() + '/drive/My Drive/My Final Year Project Folder 2023/'"],"metadata":{"id":"TmXQz-iakft3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get I1 images\n","path_to_I1 = path + 'I1_updated/'\n","list_dir = [int(file.split(\".\")[0]) for file in os.listdir(path_to_I1)]\n","list_dir.sort()\n","list_I1_images = []\n","for file_name in list_dir:\n","    img = cv2.imread(filename=path_to_I1 + str(file_name) + \".png\")\n","    img = cv2.cvtColor(src=img, code=cv2.COLOR_BGR2RGB)\n","    img = cv2.resize(src=img, dsize=(336, 176), interpolation=cv2.INTER_LINEAR)\n","    list_I1_images.append(img)\n","list_I1_images = np.asarray(list_I1_images)"],"metadata":{"id":"k7S2ydgdmqyu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get I1w images\n","path_to_I1w = path + 'I1w/'\n","list_dir = [int(file.split(\".\")[0]) for file in os.listdir(path_to_I1w)]\n","list_dir.sort()\n","list_I1w_images = []\n","for file_name in list_dir:\n","    img = cv2.imread(filename=path_to_I1w + str(file_name) + \".png\")\n","    img = cv2.cvtColor(src=img, code=cv2.COLOR_BGR2RGB)\n","    img = cv2.resize(src=img, dsize=(336, 176), interpolation=cv2.INTER_LINEAR)\n","    list_I1w_images.append(img)\n","list_I1w_images = np.asarray(list_I1w_images)"],"metadata":{"id":"drJaCWLMmQ5O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Get I2 images\n","path_to_I2 = path + 'I2_updated/'\n","list_dir = [int(file.split(\".\")[0]) for file in os.listdir(path_to_I2)]\n","list_dir.sort()\n","list_I2_images = []\n","for file_name in list_dir:\n","    img = cv2.imread(filename=path_to_I2 + str(file_name) + \".png\")\n","    img = cv2.cvtColor(src=img, code=cv2.COLOR_BGR2RGB)\n","    img = cv2.resize(src=img, dsize=(336, 176), interpolation=cv2.INTER_LINEAR)\n","    list_I2_images.append(img)\n","list_I2_images = np.asarray(list_I2_images)"],"metadata":{"id":"GS_3ph_kkinP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Translation between camera frames"],"metadata":{"id":"yXAHSH6myRzp"}},{"cell_type":"code","source":["translation_between_frames = [0, 1, 1.25, 1.3, 1.85, 2.25, 3.78, 1.593, 1.3275, 1.239, 1.416, 0.885, 1.593, 3.009, 2.655, 1.239, 1.77, 1.46, 0.708, 1.593,\n","                              1.77, 1.593, 1.239, 1.593, 1.947, 1.77, 1.416, 1.239, 1.239, 1.062, 2.124, 2.301, 2.655, 2.124, 1.947, 1.062, 1.416, 1.239, \n","                              1.593, 1.416, 2.124, 4.779, 1.239, 1.593, 1.593, 1.77, 1.239, 1.416, 1.593, 1.593, 2.124, 1.77, 3.009, 1.947, 1.593, 2.124,\n","                              1.947, 1.062, 2.301, 2.124]\n","translation_between_frames = np.asarray(a=translation_between_frames)"],"metadata":{"id":"z39GClH4yVkX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Definition of epipole generator function"],"metadata":{"id":"jn5wsonvne2A"}},{"cell_type":"code","source":["def epipole_generator(ndarray_I1w, ndarray_I2):\n","    epipole_list = []\n","    grouped_data = tf.data.Dataset.from_tensor_slices((ndarray_I1w, ndarray_I2))\n","\n","    for element in grouped_data:\n","        I1w, I2 = element\n","        I1w = I1w.numpy()\n","        I2 = I2.numpy()\n","\n","        sift = cv2.SIFT_create()\n","        # find the keypoints and descriptors with SIFT\n","        kp1, des1 = sift.detectAndCompute(I1w,None)\n","        kp2, des2 = sift.detectAndCompute(I2,None)\n","\n","        # FLANN parameters\n","        FLANN_INDEX_KDTREE = 1\n","        index_params = dict(algorithm = FLANN_INDEX_KDTREE, trees = 5)\n","        search_params = dict(checks=50)\n","        flann = cv2.FlannBasedMatcher(index_params,search_params)\n","        matches = flann.knnMatch(des1,des2,k=2)\n","        pts1 = []\n","        pts2 = []\n","\n","        # ratio test as per Lowe's paper\n","        for i,(m,n) in enumerate(matches):\n","            if m.distance < 0.8*n.distance:\n","                pts2.append(kp2[m.trainIdx].pt)\n","                pts1.append(kp1[m.queryIdx].pt)\n","\n","        # Now we have the list of best matches from both the images. Let's find the Fundamental Matrix.\n","        pts1 = np.int32(pts1)\n","        pts2 = np.int32(pts2)\n","        F, mask = cv2.findFundamentalMat(pts1,pts2, cv2.FM_LMEDS)\n","\n","        # We select only inlier points\n","        pts1 = pts1[mask.ravel()==1]\n","        pts2 = pts2[mask.ravel()==1]\n","\n","        # Find epilines corresponding to points in I2 (second image) and\n","        # drawing its lines on I1w\n","        lines1 = cv2.computeCorrespondEpilines(pts2.reshape(-1,1,2), 2,F)\n","        lines1 = lines1.reshape(-1,3)\n","\n","        # Find epilines corresponding to points in I1w (first image)\n","        # drawing its lines on I2\n","        lines2 = cv2.computeCorrespondEpilines(pts1.reshape(-1,1,2), 1,F)\n","        lines2 = lines2.reshape(-1,3)\n","        \n","        epipole_left = np.cross(lines1[0], lines1[1])\n","        epipole_right = np.cross(lines2[0], lines2[1])\n","\n","        if epipole_left[2] != 0:\n","            epipole_left = epipole_left/epipole_left[2]\n","        else:\n","            epipole_left = epipole_left\n","            \n","        if epipole_right[2] != 0:\n","            epipole_right = epipole_right/epipole_right[2]\n","        else:\n","            epipole_right = epipole_right\n","        \n","        epipole_list.append(epipole_right)\n","\n","    # The epipoles are returned in homogenous coordinates\n","    \n","    return np.asarray(epipole_list)"],"metadata":{"id":"suXWhtA6LfGO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Definition of Optical flow generator function"],"metadata":{"id":"dHlpGYyinrl8"}},{"cell_type":"code","source":["@tf.function\n","def optical_flow_generator(struct_param_tensor, epipole_tensor, translation_between_frames):\n","    \"\"\"\n","    @param struct_param_tensor is a tensor of shape [1, 176, 336, 1]\n","    @param epipole_tensor is a tensor of shape [3, ]\n","    @returns optical flow a tensor of [1, 176, 336, 2]\n","    \"\"\"\n","    epipole = [epipole_tensor[0], epipole_tensor[1]]\n","    struct_param_tensor_copy = tf.identity(struct_param_tensor)\n","    struct_param_tensor_copy = tf.cast(struct_param_tensor_copy, tf.float32)  # Convert to float32\n","    height_of_camera = .152 # in meters\n","    translation_between_frames = tf.cast(x=translation_between_frames, dtype=tf.float32)\n","    b = translation_between_frames/height_of_camera\n","    \n","    # i_range represents the height dimension in struct_param_tensor_copy\n","    i_range = tf.range(struct_param_tensor_copy.shape[1], dtype=tf.int32)\n","\n","    # j_range represents the width dimension in struct_param_tensor_copy\n","    j_range = tf.range(struct_param_tensor_copy.shape[2], dtype=tf.int32)\n","\n","    I, J = tf.meshgrid(i_range, j_range, indexing='ij')\n","\n","    # Assuming you have the tensors struct_param, I, and J defined\n","    \n","    # Modify the range of values in I and J to match the valid indices\n","    I_modified = tf.clip_by_value(I, 0, struct_param_tensor_copy.shape[1] - 1)\n","    J_modified = tf.clip_by_value(J, 0, struct_param_tensor_copy.shape[2] - 1)\n","    \n","    # Expand dimensions of I_modified and J_modified to match struct_param\n","    I_expanded = tf.expand_dims(I_modified, axis=-1)  # Shape: [176, 336, 1]\n","    J_expanded = tf.expand_dims(J_modified, axis=-1)  # Shape: [176, 336, 1]\n","    \n","    # Use tf.concat to create indices tensor\n","    indices = tf.concat([I_expanded, J_expanded], axis=-1)  # Shape: [176, 336, 2]\n","    \n","    # Use tf.gather_nd to access elements of struct_param based on indices\n","    result = tf.gather_nd(struct_param_tensor_copy[0], indices)[:,:,0]  # Shape: [176, 336, 1]\n","    \n","    new_u_x = ((tf.math.multiply(x=result, y=b)) / (tf.math.multiply(x=result, y=b) - 1)) * (tf.cast(epipole[0], tf.float32) - tf.cast(J, tf.float32)) # Shape: [176, 336]\n","    new_u_y = ((tf.math.multiply(x=result, y=b)) / (tf.math.multiply(x=result, y=b) - 1)) * (tf.cast(epipole[1], tf.float32) - tf.cast(I, tf.float32)) # Shape: [176, 336]\n","\n","    new_u_x = tf.reshape(new_u_x, (176, 336, 1))\n","    new_u_y = tf.reshape(new_u_y, (176, 336, 1))\n","    \n","    return tf.expand_dims(tf.concat([new_u_x, new_u_y], axis=2), axis=0)"],"metadata":{"id":"5hXQw4CrpEGD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Definition of the optical flow warp function"],"metadata":{"id":"Q4tCxPOXLC6V"}},{"cell_type":"code","source":["@tf.function\n","def optical_flow_warp(optical_flow_tensor, I1w_tensor):\n","    \"\"\" \n","    @param optical_flow_tensor is a tensor of shape [1, 480, 640, 2]\n","    @param I1w_tensor is a tensor of shape [1, 480, 640, 3]\n","    @returns warped image a tensor of [1, 480, 640, 3]\n","    \n","    \"\"\"\n","    height  = tf.shape(optical_flow_tensor)[1]\n","    width = tf.shape(optical_flow_tensor)[2]\n","\n","    flow_x,flow_y = tf.split(optical_flow_tensor,[1,1], axis =3)\n","    coord_x, coord_y = tf.meshgrid(tf.range(width), tf.range(height))\n","    pos_x = tf.cast(tf.expand_dims(tf.expand_dims(coord_x, axis = 2), axis=0), dtype=tf.float32) + flow_x\n","    pos_y = tf.cast(tf.expand_dims(tf.expand_dims(coord_y, axis = 2), axis=0), dtype=tf.float32) + flow_y\n","\n","    warped_points = tf.concat([pos_x, pos_y], axis=3, name='warped_points')\n","    I1w_tensor = tf.cast(I1w_tensor, dtype=tf.float32)\n","\n","    return tfa.image.resampler(I1w_tensor, warped_points)"],"metadata":{"id":"txN1RaSS0Zo4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4677ae0a"},"source":["# Create neural network\n","## Function definitions of building blocks for an autoencoder"]},{"cell_type":"code","source":["# This code snippet was found here: https://youtu.be/azM57JuQpQI?list=PLZsOBAyNTZwbR08R959iCvYT3qzhxvGOE\n","#Convolutional block to be used in autoencoder and U-Net\n","def conv_block(input_representation, num_filters):\n","    x = Conv2D(num_filters, 3, padding=\"same\")(input_representation)\n","    x = BatchNormalization()(x)   #Not in the original network. \n","    x = Activation(\"relu\")(x)\n","\n","    x = Conv2D(num_filters, 3, padding=\"same\")(x)\n","    x = BatchNormalization()(x)  #Not in the original network\n","    x = Activation(\"relu\")(x)\n","\n","    return x\n","\n","#Encoder block: Conv block followed by maxpooling\n","def encoder_block(input_representation, num_filters):\n","    x = conv_block(input_representation, num_filters)\n","    p = MaxPool2D((2, 2))(x)\n","    return x, p   \n","\n","#Decoder block for autoencoder (no skip connections)\n","def decoder_block(input_representation, num_filters):\n","    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input_representation)\n","    x = conv_block(x, num_filters)\n","    return x"],"metadata":{"id":"z511y7PK0hcS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fd330065"},"source":["## Functional definition for an encoder"]},{"cell_type":"code","source":["# This code snippet was found here: https://youtu.be/azM57JuQpQI?list=PLZsOBAyNTZwbR08R959iCvYT3qzhxvGOE\n","#Encoder will be the same for Autoencoder and U-net\n","#We are getting both conv output and maxpool output for convenience.\n","#we will ignore conv output for Autoencoder. It acts as skip connections for U-Net\n","def build_encoder(input_representation):\n","    #inputs = Input(input_shape)\n","\n","    s1, p1 = encoder_block(input_representation, 64)\n","    s2, p2 = encoder_block(p1, 128)\n","    s3, p3 = encoder_block(p2, 256)\n","    s4, p4 = encoder_block(p3, 512)\n","    \n","    encoded = conv_block(p4, 1024) #Bridge\n","    \n","    return encoded"],"metadata":{"id":"bATHW1t40k22"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2291e475"},"source":["## Functional definition for a decoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aaa9a3fa"},"outputs":[],"source":["# This code snippet was found here: https://youtu.be/azM57JuQpQI?list=PLZsOBAyNTZwbR08R959iCvYT3qzhxvGOE\n","#Decoder for Autoencoder ONLY. \n","def build_decoder(encoded):\n","    d1 = decoder_block(encoded, 512)\n","    d2 = decoder_block(d1, 256)\n","    d3 = decoder_block(d2, 128)\n","    d4 = decoder_block(d3, 64)\n","    \n","    decoded = Conv2D(1, 3, padding=\"same\", activation=\"relu\")(d4)\n","    return decoded"]},{"cell_type":"markdown","metadata":{"id":"5a146c57"},"source":["## Functional definition of an Autoencoder"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5a129df"},"outputs":[],"source":["# This code snippet was found here: https://youtu.be/azM57JuQpQI?list=PLZsOBAyNTZwbR08R959iCvYT3qzhxvGOE\n","#Use encoder and decoder blocks to build the autoencoder. \n","def build_autoencoder(input_shape):\n","    input_img = Input(shape=input_shape)\n","    autoencoder = Model(input_img, build_decoder(build_encoder(input_img)))\n","    return(autoencoder)"]},{"cell_type":"markdown","source":["# Functional definition of U-Net"],"metadata":{"id":"vbcYOnfy7Hb4"}},{"cell_type":"code","source":["# This code snippet was found here: https://youtu.be/azM57JuQpQI?list=PLZsOBAyNTZwbR08R959iCvYT3qzhxvGOE\n","#Decoder block for unet\n","#skip features gets input from encoder for concatenation\n","def decoder_block_for_unet(input_representation, skip_features, num_filters):\n","    x = Conv2DTranspose(num_filters, (2, 2), strides=2, padding=\"same\")(input_representation)\n","    x = Concatenate()([x, skip_features])\n","    x = conv_block(x, num_filters)\n","    return x\n","\n","#Build Unet using the blocks\n","def build_unet(input_shape):\n","    inputs = Input(input_shape)\n","\n","    s1, p1 = encoder_block(inputs, 64)\n","    s2, p2 = encoder_block(p1, 128)\n","    s3, p3 = encoder_block(p2, 256)\n","    s4, p4 = encoder_block(p3, 512)\n","\n","    b1 = conv_block(p4, 1024) #Bridge\n","\n","    d1 = decoder_block_for_unet(b1, s4, 512)\n","    d2 = decoder_block_for_unet(d1, s3, 256)\n","    d3 = decoder_block_for_unet(d2, s2, 128)\n","    d4 = decoder_block_for_unet(d3, s1, 64)\n","\n","    outputs = Conv2D(1, 1, padding=\"same\", activation=\"relu\")(d4)\n","\n","    model = Model(inputs, outputs, name=\"U-Net\")\n","    return model"],"metadata":{"id":"WZpeW0KI7KSk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Model"],"metadata":{"id":"NalmiKc-T8Jb"}},{"cell_type":"code","source":["model = build_autoencoder((176, 336, 3))"],"metadata":{"id":"i9zVv8Tx1ATl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Training Loop"],"metadata":{"id":"79P822ss1Jai"}},{"cell_type":"code","source":["# Define the Robust Charbonnier Photometric loss\n","class PhotometricLoss(tf.keras.losses.Loss):\n","    # initialize instance attributes\n","    def __init__(self, epsilon=0.1, reduction=tf.keras.losses.Reduction.AUTO):\n","        super().__init__(reduction=reduction)\n","        self.epsilon = epsilon\n","    # Compute loss\n","    def call(self, y_true, y_pred):\n","        y_true = tf.cast(y_true, tf.float32)  # Convert y_true to float32\n","        y_pred = tf.cast(y_pred, tf.float32)  # Convert y_pred to float32\n","        delta = y_true - y_pred\n","        return tf.math.reduce_mean(tf.math.sqrt(tf.math.square(delta) + tf.math.square(self.epsilon)))"],"metadata":{"id":"RP_8arKD1OOK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This code snippet was found here: https://github.com/daniilidis-group/EV-FlowNet\n","@tf.function\n","def charbonnier_loss(delta, alpha=0.5, epsilon=0.1):\n","    loss = tf.reduce_mean(tf.pow(tf.math.square(delta) + tf.math.square(epsilon), alpha))\n","    return loss"],"metadata":{"id":"tmAO4sIt1Ubw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# This code snippet was found here: https://github.com/daniilidis-group/EV-FlowNet\n","@tf.function\n","def compute_smoothness_loss(struct_param):\n","    struct_param_ucrop = struct_param[..., 1:, :]\n","    struct_param_dcrop = struct_param[..., :-1, :]\n","    struct_param_lcrop = struct_param[:, 1:, ...]\n","    struct_param_rcrop = struct_param[:, :-1, ...]\n","\n","    struct_param_ulcrop = struct_param[:, 1:, 1:, :]\n","    struct_param_drcrop = struct_param[:, :-1, :-1, :]\n","    struct_param_dlcrop = struct_param[:, :-1, 1:, :]\n","    struct_param_urcrop = struct_param[:, 1:, :-1, :]\n","\n","    smoothness_loss = charbonnier_loss(struct_param_lcrop - struct_param_rcrop) + charbonnier_loss(struct_param_ucrop - struct_param_dcrop) + charbonnier_loss(struct_param_ulcrop - struct_param_drcrop) + charbonnier_loss(struct_param_dlcrop - struct_param_urcrop)\n","    smoothness_loss /= 4.\n","    return smoothness_loss"],"metadata":{"id":"fdMkmjDG1Z-y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Instantiate an optimizer.\n","optimizer = Adam(learning_rate=0.00001)\n","# Instantiate a loss function.\n","loss_fn = PhotometricLoss()"],"metadata":{"id":"cbz7X8701bTz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["dataset_voxel = np.load(path + \"voxel_grids.npy\")"],"metadata":{"id":"JOag-AHq1ed3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epipoles = epipole_generator(list_I1_images, list_I2_images)"],"metadata":{"id":"r7HhHgWM1m_Q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["event_volumes = tf.keras.utils.normalize(x=dataset_voxel[:60, :176, :336, :])"],"metadata":{"id":"YiVO0jhn1rdJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translation_between_frames = tf.expand_dims(translation_between_frames, axis=0)"],"metadata":{"id":"_BYX7M2y18ut"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["translation_between_frames = tf.broadcast_to(input=translation_between_frames, shape=[60, 60])"],"metadata":{"id":"zlnVnZ683ozY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = tf.data.Dataset.from_tensor_slices((event_volumes, list_I1w_images, list_I2_images, epipoles, translation_between_frames))"],"metadata":{"id":"h-lEpwcu4aJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["epochs = 50\n","for epoch in range(epochs):\n","    print(\"\\nStart of epoch %d\" % (epoch,))\n","    hold_loss = []\n","     # Iterate over the dataset.\n","    for step, element in enumerate(train_dataset):\n","        first_element, second_element, third_element, fourth_element, fifth_element = element\n","        voxel = tf.Variable(initial_value=first_element, trainable=True)\n","        Iw1_im = tf.Variable(initial_value=second_element, trainable=True)\n","        I2_im = tf.Variable(initial_value=third_element, trainable=True)\n","        I2_im_epipole = tf.Variable(initial_value=fourth_element, trainable=True)\n","\n","        # Open a GradientTape to record the operations run\n","        # during the forward pass, which enables auto-differentiation.\n","        with tf.GradientTape() as tape:\n","            # Run the forward pass of the layer.\n","            # The operations that the layer applies\n","            # to its inputs are going to be recorded\n","            # on the GradientTape.\n","\n","            tape.watch(model.trainable_weights)\n","            structure_parameter = model(tf.expand_dims(input=voxel, axis=0), training=True)  # Logits for this minibatch\n","\n","            #structure has shape [1, 480, 640, 1]\n","\n","            Iw1_im = tf.expand_dims(input=Iw1_im, axis=0)\n","            I2_im = tf.expand_dims(input=I2_im, axis=0)\n","\n","            # Both I1w and I2 images have shape [1, 480, 640, 3]\n","\n","            op_flow = optical_flow_generator(structure_parameter, I2_im_epipole, fifth_element[step])\n","\n","            # optical flow a tensor of [1, 480, 640, 2]\n","\n","            I2_hat = optical_flow_warp(op_flow, Iw1_im)\n","\n","            # Compute the loss value for this minibatch.\n","            loss_value = loss_fn(I2_im, I2_hat) + 0.2*compute_smoothness_loss(structure_parameter)\n","\n","        # Use the gradient tape to automatically retrieve\n","        # the gradients of the trainable variables with respect to the loss.\n","        grads = tape.gradient(loss_value, model.trainable_weights, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n","\n","        # Run one step of gradient descent by updating\n","        # the value of the variables to minimize the loss.\n","        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n","        hold_loss.append(loss_value)\n","    print('Average loss', \":\", tf.math.reduce_mean(tf.convert_to_tensor(hold_loss)).numpy())\n","    #print(loss_value)"],"metadata":{"id":"yTVpWgzmjcqQ"},"execution_count":null,"outputs":[]}]}